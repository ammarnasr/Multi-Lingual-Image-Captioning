# 
 Multi-Lingual-Image-Captioning using Clip for CW4



# Multi-Lingual-Image-Captioning.

Google Colab Notebook: <a https://colab.research.google.com/drive/10lxPrsQFSLQGrmsTk5p6Zb2Mj1v9Ky-N?usp=sharing"><img src="https://colab.research.google.com/assets/colab-badge.svg" height=20></a>  





## Implementation for  Multi-Lingual-Image-Captioning using Pre-Trained Models for MLP-CW4




## Description  
This paper proposes a method for training image captioning networks on languages other than English using pre-trained models and a transformer network. We employ CLIP as our image encoder and fine-tune different versions of the GPT-2 language model as decoders for our target language. We evaluate our approach on English, German, and Arabic, achieving comparable results to state-of-the-art models even with smaller datasets. Our proposed method has implications for bridging the gap between English and other language image captioning, enabling more applications to benefit from this technology.

## Examples
![alt text](https://github.com/ammarnasr/Multi-Lingual-Image-Captioning/raw/main/readMeImgs/qa2.png)

## Training prerequisites

[comment]: <> (Dependencies can be found at the [Google Colab notebook]&#40;https://colab.research.google.com/drive/1dOqoCRqcan56fOOPNTLFQcBtg0ILO4wL?usp=sharing&#41; )
