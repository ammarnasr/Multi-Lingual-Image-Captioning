Here is a draft GitHub README.md for the code of the image captioning paper:

# Multi-Lingual Image Captioning

This repo contains the code for the paper "Image Captioning Unified (ICU): A Resource-Efficient Approach to Image Captioning". 

## Overview

This paper explores image captioning in multiple languages, including English, German, and Arabic, using a resource-efficient approach with pre-trained models. The method utilizes CLIP as the image encoder, fine-tuned GPT-2 models as decoders for each language, and trains a small transformer network adapter to adapt CLIP embeddings to GPT-2 prefixes for caption generation.

## Requirements

- Python 3.6+
- PyTorch 1.7+
- Transformers
- CLIP
- Other common packages (numpy, sklearn, matplotlib, etc)

## Datasets

The following datasets are used:

- Multi30k (English, German)
- Flickr8k-Arabic (Arabic)
Here is a section you can add for sample images and captions:

## Samples

Here are some sample images with captions generated by our models:

![Image1](samples/image1.jpg)



## Usage

The main scripts are:

- `train.py` - trains the adapter model for a specified language
- `eval.py` - evaluates a trained model on a dataset
- `sample.py` - generates sample captions for images using a trained model

Modify the config in `config.py` to specify model hyperparameters and data paths.

Pre-trained adapter models can be downloaded from [MODEL_LINK].

## Reference

```
@article{ICU2022,
  title={Image Captioning Unified (ICU): A Resource-Efficient Approach to Image Captioning},
  author={John Doe, Jane Doe, Bob Doe},
  journal={arXiv preprint arXiv:0000.00000},
  year={2022}
}
```

## License

This project is licensed under the MIT License
