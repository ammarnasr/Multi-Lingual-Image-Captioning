{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "azoxanaYBMOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsOqabGkz3iy"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade --no-cache-dir gdown\n",
        "!pip -q install transformers\n",
        "!pip -q install multilingual-clip\n",
        "!pip -q install ftfy regex tqdm\n",
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "!pip -q install pandas -U\n",
        "!pip -q install arabic-reshaper\n",
        "!pip -q install python-bidi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ammarnasr/Multi-Lingual-Image-Captioning.git\n",
        "\n",
        "### Download Training Data ###\n",
        "!gdown --id 1sHQMEEFOjI0TrJW0eE5diYWNM4xOOxRN \n",
        "!unzip -qq /content/data.zip -d /content/Multi-Lingual-Image-Captioning\n",
        "!rm /content/data.zip\n",
        "\n",
        "### Download Latest Check points ###\n",
        "!gdown --id 1V6SYQWQWWOEWQfmXhYH4u-xCNdj4bv97\n",
        "!unzip -qq /content/checkpoints.zip -d /content/Multi-Lingual-Image-Captioning\n",
        "!rm /content/checkpoints.zip\n"
      ],
      "metadata": {
        "id": "E-NpDsVxmOyE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "x6jKSm8N0UgO"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "import os\n",
        "os.chdir('/content/Multi-Lingual-Image-Captioning')\n",
        "os.getcwd()\n",
        "\n",
        "\n",
        "\n",
        "def copy_ckpts():  \n",
        "  '''Copy Saved Checkpoints from Colab to drive/MyDrive/Multi-Lingual-Image-Captioning/checkpoints'''\n",
        "  checkpoints_dir = './checkpoints'\n",
        "  checkpoints_dir_drive = '/content/drive/MyDrive/Multi-Lingual-Image-Captioning/checkpoints'\n",
        "  ckpt_files = [f for f in listdir(checkpoints_dir) if isfile(join(checkpoints_dir, f))]\n",
        "  for ckpt_file in ckpt_files :\n",
        "    src = join(checkpoints_dir, ckpt_file)\n",
        "    dst = join(checkpoints_dir_drive, ckpt_file)\n",
        "    shutil.copyfile(src, dst)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocess\n",
        "# from dataset import ClipGPTFlickr8kDataset\n",
        "\n",
        "# # Preprocess Data (Data already saved to drive no need to run)\n",
        "# preprocess.create_CLIP_embeddings_for_images(lang='arabic')\n",
        "# preprocess.create_CLIP_embeddings_for_images(lang='english')\n",
        "# dataset = ClipGPTFlickr8kDataset('/content/Multi-Lingual-Image-Captioning/data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', 10)\n",
        "# dataset = ClipGPTFlickr8kDataset('/content/Multi-Lingual-Image-Captioning/data/embeddings/english_CLIP-ViT-B-32_embeddings.pkl', 10)\n"
      ],
      "metadata": {
        "id": "CX4p91blCw51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Edit The Parameters and run the code to start exprimpent  #####\n",
        "exp_args = '''\n",
        "\n",
        "#create a class called demo args to store the arguments\n",
        "class DemoArgs:\n",
        "    def __init__(self):\n",
        "        self.data = './data/embeddings/english_CLIP-ViT-B-32_embeddings.pkl'\n",
        "        self.lang = 'english'\n",
        "        self.out_dir = './checkpoints'\n",
        "        self.output_prefix = 'english_exp_1'\n",
        "        self.epochs = 30\n",
        "        self.save_every = 3\n",
        "        self.prefix_length = 10\n",
        "        self.prefix_length_clip = 10\n",
        "        self.bs = 40\n",
        "        self.only_prefix = True\n",
        "        self.mapping_type = 'transformer'\n",
        "        self.num_layers = 8\n",
        "        self.is_rn = False\n",
        "        self.normalize_prefix = False\n",
        "\n",
        "'''\n",
        "with open('args.py', 'w') as f:\n",
        "    f.write(exp_args)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Train From Scratch ###\n",
        "# !python train_gpt.py \n",
        "\n",
        "### Contiue from avialabe checkpoint ###\n",
        "!python train_gpt.py ./checkpoints/english_exp_1-029.pt\n",
        "\n",
        "# copy_ckpts()\n",
        "\n"
      ],
      "metadata": {
        "id": "R-79aW-M8ao6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######## Edit The Parameters and run the code to start exprimpent  #####\n",
        "exp_args = '''\n",
        "\n",
        "#create a class called demo args to store the arguments\n",
        "class DemoArgs:\n",
        "    def __init__(self):\n",
        "        self.data = './data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl'\n",
        "        self.lang = 'arabic'\n",
        "        self.out_dir = './checkpoints'\n",
        "        self.output_prefix = 'arabic_exp_1'\n",
        "        self.epochs = 30\n",
        "        self.save_every = 3\n",
        "        self.prefix_length = 10\n",
        "        self.prefix_length_clip = 10\n",
        "        self.bs = 40\n",
        "        self.only_prefix = True\n",
        "        self.mapping_type = 'transformer'\n",
        "        self.num_layers = 8\n",
        "        self.is_rn = False\n",
        "        self.normalize_prefix = False\n",
        "\n",
        "'''\n",
        "with open('args.py', 'w') as f:\n",
        "    f.write(exp_args)\n",
        "    \n",
        "    \n",
        "\n",
        "\n",
        "### Train From Scratch ###\n",
        "# !python train_gpt.py \n",
        "\n",
        "### Contiue from avialabe checkpoint ###\n",
        "!python train_gpt.py ./checkpoints/arabic_exp_1-029.pt\n",
        "\n",
        "\n",
        "# copy_ckpts()"
      ],
      "metadata": {
        "id": "xEI6MwTNiDMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296d1af3-86fa-438b-84ee-008d9f98ad8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-03 11:30:06.868828: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-03-03 11:30:07.718483: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-03 11:30:07.718584: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-03 11:30:07.718603: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Loading model from checkpoint\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:1248: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  warnings.warn(\n",
            "Downloading (…)lve/main/config.json: 100% 666/666 [00:00<00:00, 112kB/s]\n",
            "Downloading (…)\"pytorch_model.bin\";: 100% 510M/510M [00:09<00:00, 51.6MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 30.0/30.0 [00:00<00:00, 14.2kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 1.55M/1.55M [00:00<00:00, 82.9MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 1.21M/1.21M [00:00<00:00, 60.6MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 120/120 [00:00<00:00, 59.1kB/s]\n",
            "Data size is 24273\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            ">>> Training epoch 30 out of 60\n",
            "arabic_exp_1:   7% 42/606 [00:26<05:30,  1.71it/s, loss=2.11]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR3xeRrP-FS6"
      },
      "outputs": [],
      "source": [
        "##########Push Updates to Github #################\n",
        "# !git add .\n",
        "# !git config --global user.email \"ammarnasraza@gmail.com\"\n",
        "# !git config --global user.name \"ammarnasr\"\n",
        "# !git commit -m 'from colab'\n",
        "# !git remote set-url origin https://ammarnasr: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@github.com/ammarnasr/Multi-Lingual-Image-Captioning.git\n",
        "# !git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sYHlm12A63S"
      },
      "outputs": [],
      "source": [
        "######## Run Inference On Sample Images ################\n",
        "# import inference_gpt\n",
        "# ckpt_path = '/content/Multi-Lingual-Image-Captioning/checkpoints/arabic_exp_1-029.pt'\n",
        "# inference_gpt.main(ckpt_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Zip CKPTs to a file of the expriment ######\n",
        "# import os\n",
        "# lang = '<Expirement Lang>'\n",
        "# os.mkdri(f'/content/{lang}')\n",
        "# !zip -r /content/<Expriement Lang>/<Expriment Name>.zip /content/Multi-Lingual-Image-Captioning/checkpoints/\n",
        "# !cp    /content/<Expriement Lang>/<Expriment Name>.zip /content/drive/MyDrive/Multi-Lingual-Image-Captioning/checkpoints"
      ],
      "metadata": {
        "id": "UVyBusdk4ife"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}