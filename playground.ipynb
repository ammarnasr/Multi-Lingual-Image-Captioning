{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import clip\n",
    "import torch\n",
    "import pickle\n",
    "import PIL.Image \n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as nnf\n",
    "from plotting import fix_arabic_text\n",
    "from models import  ClipCaptionPrefix\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def beam_search(model, tokenizer, embed, entry_length=20, top_p=0.8, temperature=1., stop_token= '.'):\n",
    "    '''Beam search for the GPT model.'''\n",
    "    \n",
    "    model.eval()\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    generated = embed\n",
    "    tokens = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(entry_length):\n",
    "            \n",
    "            #  get the logits for the next token\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            print(f'shape of outputs: {outputs.shape}')\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[:, indices_to_remove] = filter_value\n",
    "            \n",
    "            #  take the most likely token and add it to the sequence\n",
    "            next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "\n",
    "\n",
    "            # transform the token to embedding\n",
    "            next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "\n",
    "            # add the token to the sequence\n",
    "            if tokens is None:\n",
    "                tokens = next_token\n",
    "            else:\n",
    "                tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            \n",
    "            # add the embedding to the sequence\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "\n",
    "            # stop if the stop token is reached\n",
    "            if stop_token_index == next_token.item():\n",
    "                break\n",
    "            if stop_token == tokenizer.decode(tokens.squeeze().cpu().numpy())[-1]:\n",
    "                break\n",
    "\n",
    "        # convert the sequence to text\n",
    "        output_list = list(tokens.squeeze().cpu().numpy())\n",
    "        output_text = tokenizer.decode(output_list)\n",
    "        generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "\n",
    "\n",
    "def generate_caption(image_path, model, preprocess, clip_model, tokenizer ,prefix_length,  lang ,device):\n",
    "    \n",
    "\n",
    "\n",
    "def main(model_path):\n",
    "    #Read the language from the model path\n",
    "    if 'arabic' in model_path:\n",
    "        lang = 'arabic'\n",
    "    if 'english' in model_path:\n",
    "        lang = 'english'\n",
    "    print(f'The Lang is {lang}')\n",
    "    # Load the CLIP model\n",
    "    device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    \n",
    "    # Load the GPT model Tokenizer\n",
    "    if lang == 'arabic':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"akhooli/gpt2-small-arabic\")\n",
    "    if lang == 'english':\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    # Load the GPT model\n",
    "    model, prefix_length = load_model(model_path)\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    sample_images_dir = './sample_image'\n",
    "    sample_images_paths = [os.path.join(sample_images_dir, image_name) for image_name in os.listdir(sample_images_dir)]\n",
    "    for image_path in sample_images_paths:\n",
    "        generate_caption(image_path, model,preprocess, clip_model, tokenizer, prefix_length, lang, device)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Read the model path from the command line\n",
    "    main(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import clip\n",
    "import torch\n",
    "import pickle\n",
    "import PIL.Image \n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as nnf\n",
    "from plotting import fix_arabic_text\n",
    "from models import  ClipCaptionPrefix\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "def load_model(model_path):\n",
    "    '''load model from path'''\n",
    "    args_path = model_path.replace('.pt', '_args.pkl')\n",
    "    with open(args_path, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    model = ClipCaptionPrefix(\n",
    "        prefix_length=args.prefix_length,\n",
    "        lang = args.lang ,\n",
    "        clip_length=args.prefix_length_clip,\n",
    "        prefix_size=512,\n",
    "        num_layers=args.num_layers,\n",
    "        mapping_type=args.mapping_type)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    return model , args.prefix_length\n",
    "\n",
    "\n",
    "\n",
    "model_path = './checkpoints/english_exp_1-029.pt'\n",
    "#Read the language from the model path\n",
    "if 'arabic' in model_path:\n",
    "    lang = 'arabic'\n",
    "if 'english' in model_path:\n",
    "    lang = 'english'\n",
    "print(f'The Lang is {lang}')\n",
    "# Load the CLIP model\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Load the GPT model Tokenizer\n",
    "if lang == 'arabic':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"akhooli/gpt2-small-arabic\")\n",
    "if lang == 'english':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT model\n",
    "model, prefix_length = load_model(model_path)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "sample_images_dir = './sample_image'\n",
    "sample_images_paths = [os.path.join(sample_images_dir, image_name) for image_name in os.listdir(sample_images_dir)]\n",
    "image_path = sample_images_paths[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = io.imread(image_path)\n",
    "pil_image = PIL.Image.fromarray(image)\n",
    "image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_beam(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "import numpy as np\n",
    "beam_size: int = 5\n",
    "embed= prefix_embed\n",
    "prompt=None\n",
    "entry_length=67\n",
    "temperature=1.0\n",
    "stop_token: str = \".\"\n",
    "\n",
    "\n",
    "model.eval()\n",
    "stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "tokens = None\n",
    "scores = None\n",
    "device = next(model.parameters()).device\n",
    "seq_lengths = torch.ones(beam_size, device=device)\n",
    "is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "with torch.no_grad():\n",
    "    if embed is not None:\n",
    "        generated = embed\n",
    "    else:\n",
    "        if tokens is None:\n",
    "            tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "            tokens = tokens.unsqueeze(0).to(device)\n",
    "            generated = model.gpt.transformer.wte(tokens)\n",
    "    for i in range(entry_length):\n",
    "        outputs = model.gpt(inputs_embeds=generated)\n",
    "        print(f'generated shape is {generated.shape}')\n",
    "        logits = outputs.logits\n",
    "        print(f'logits shape is {logits.shape}')\n",
    "        logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "        logits = logits.softmax(-1).log()\n",
    "        if scores is None:\n",
    "            scores, next_tokens = logits.topk(beam_size, -1)\n",
    "            generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "            next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "            if tokens is None:\n",
    "                tokens = next_tokens\n",
    "            else:\n",
    "                tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "        else:\n",
    "            logits[is_stopped] = -float(np.inf)\n",
    "            logits[is_stopped, 0] = 0\n",
    "            scores_sum = scores[:, None] + logits\n",
    "            seq_lengths[~is_stopped] += 1\n",
    "            scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "            scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(\n",
    "                beam_size, -1\n",
    "            )\n",
    "            next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "            seq_lengths = seq_lengths[next_tokens_source]\n",
    "            next_tokens = next_tokens % scores_sum.shape[1]\n",
    "            next_tokens = next_tokens.unsqueeze(1)\n",
    "            tokens = tokens[next_tokens_source]\n",
    "            tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            generated = generated[next_tokens_source]\n",
    "            scores = scores_sum_average * seq_lengths\n",
    "            is_stopped = is_stopped[next_tokens_source]\n",
    "        next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(\n",
    "            generated.shape[0], 1, -1\n",
    "        )\n",
    "        generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "        is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "        if is_stopped.all():\n",
    "            break\n",
    "scores = scores / seq_lengths\n",
    "output_list = tokens.cpu().numpy()\n",
    "output_texts = [\n",
    "    tokenizer.decode(output[: int(length)])\n",
    "    for output, length in zip(output_list, seq_lengths)\n",
    "]\n",
    "order = scores.argsort(descending=True)\n",
    "output_texts = [output_texts[i] for i in order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all outputs\n",
    "all_lines = 0\n",
    "for output in output_texts:\n",
    "    print(output)\n",
    "    #number of lines in the output\n",
    "    print(len(output.splitlines()))\n",
    "    #sum of the length of all lines\n",
    "    all_lines += len(output.splitlines())\n",
    "print(all_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape is torch.Size([1, 10, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 1])\n",
      "logits shape is torch.Size([1, 11, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50255])\n",
      "number of not removed tokens is 2\n",
      "tokens at the end of the loop shape is torch.Size([1, 2])\n",
      "logits shape is torch.Size([1, 12, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 3])\n",
      "logits shape is torch.Size([1, 13, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50253])\n",
      "number of not removed tokens is 4\n",
      "tokens at the end of the loop shape is torch.Size([1, 4])\n",
      "logits shape is torch.Size([1, 14, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50255])\n",
      "number of not removed tokens is 2\n",
      "tokens at the end of the loop shape is torch.Size([1, 5])\n",
      "logits shape is torch.Size([1, 15, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 6])\n",
      "logits shape is torch.Size([1, 16, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 7])\n",
      "logits shape is torch.Size([1, 17, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50251])\n",
      "number of not removed tokens is 6\n",
      "tokens at the end of the loop shape is torch.Size([1, 8])\n",
      "logits shape is torch.Size([1, 18, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 9])\n",
      "logits shape is torch.Size([1, 19, 50257])\n",
      "logits after temperature shape is torch.Size([1, 50257])\n",
      "sorted_logits shape is torch.Size([1, 50257])\n",
      "sorted_indices shape is torch.Size([1, 50257])\n",
      "cumulative_probs shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove shape is torch.Size([1, 50257])\n",
      "sorted_indices_to_remove after clone shape is torch.Size([1, 50257])\n",
      "indices_to_remove shape is torch.Size([50256])\n",
      "number of not removed tokens is 1\n",
      "tokens at the end of the loop shape is torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "entry_length=20\n",
    "top_p= 0.8\n",
    "temperature=1.\n",
    "stop_token= '.'\n",
    "embed=prefix_embed\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    generated = embed\n",
    "    tokens = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(entry_length):\n",
    "            \n",
    "            #  get the logits for the next token\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            print(f'logits shape is {logits.shape}')\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            print(f'logits after temperature shape is {logits.shape}')\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            print(f'sorted_logits shape is {sorted_logits.shape}')\n",
    "            print(f'sorted_indices shape is {sorted_indices.shape}')\n",
    "            cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            print(f'cumulative_probs shape is {cumulative_probs.shape}')\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            print(f'sorted_indices_to_remove shape is {sorted_indices_to_remove.shape}')\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            print(f'sorted_indices_to_remove after clone shape is {sorted_indices_to_remove.shape}')\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            print(f'indices_to_remove shape is {indices_to_remove.shape}')\n",
    "            logits[:, indices_to_remove] = filter_value\n",
    "            \n",
    "            print(f'number of not removed tokens is {torch.sum(logits != filter_value)}')\n",
    "            #  take the most likely token and add it to the sequence\n",
    "            next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "\n",
    "            \n",
    "            # transform the token to embedding\n",
    "            next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "\n",
    "            # add the token to the sequence\n",
    "            if tokens is None:\n",
    "                tokens = next_token\n",
    "            else:\n",
    "                tokens = torch.cat((tokens, next_token), dim=1)\n",
    "            \n",
    "            print(f'tokens at the end of the loop shape is {tokens.shape}')\n",
    "            \n",
    "            # add the embedding to the sequence\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "\n",
    "            # stop if the stop token is reached\n",
    "            if stop_token_index == next_token.item():\n",
    "                break\n",
    "            if stop_token == tokenizer.decode(tokens.squeeze().cpu().numpy())[-1]:\n",
    "                break\n",
    "\n",
    "        # convert the sequence to text\n",
    "        # output_list = list(tokens.squeeze().cpu().numpy())\n",
    "        # output_text = tokenizer.decode(output_list)\n",
    "        # generated_list.append(output_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits[0][32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices_to_remove.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display pil_image using plt\n",
    "plt.imshow(pil_image)\n",
    "plt.axis('off')\n",
    "print(generated_text_prefix)\n",
    "plt.title(generated_text_prefix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "from typing import  Union\n",
    "from dataset import ClipGPTFlickr8kDataset\n",
    "from models import ClipCaptionModel, ClipCaptionPrefix, MappingType\n",
    "from args import DemoArgs\n",
    "from bleu import belu_score\n",
    "\n",
    "\n",
    "args = DemoArgs()\n",
    "lr = 2e-5\n",
    "warmup_steps = 5000\n",
    "output_dir= args.out_dir\n",
    "output_prefix = args.output_prefix\n",
    "start_epoch = 0\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = args.bs\n",
    "epochs = args.epochs\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model = ClipCaptionPrefix(args.prefix_length, lang=args.lang , clip_length=args.prefix_length_clip, prefix_size=args.prefix_dim, num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "dataset = ClipGPTFlickr8kDataset(args.data, args.prefix_length,lang= args.lang, normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=False, drop_last=True)\n",
    "dataset2 = ClipGPTFlickr8kDataset('./data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', args.prefix_length,lang= 'arabic', normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader2 = DataLoader(dataset2, batch_size=3, shuffle=False, drop_last=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader))\n",
    "for epoch in range(start_epoch, epochs+start_epoch):\n",
    "    ietr_obj = iter(train_dataloader)\n",
    "    ietr_obj2 = iter(train_dataloader2)\n",
    "    print(f\">>> Training epoch {epoch} out of {epochs+start_epoch}\")\n",
    "    sys.stdout.flush()\n",
    "    number_of_batches = len(train_dataloader)\n",
    "    progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "    for idx in range(number_of_batches):\n",
    "        tokens, mask, prefix = next(ietr_obj)\n",
    "        tokens= tokens[0:3]\n",
    "        mask = mask[0:3]\n",
    "        prefix = prefix[0:3]\n",
    "        tokens2, _, prefix2 = next(ietr_obj2)\n",
    "        model.zero_grad()\n",
    "        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        tokens2 = tokens2.to(device)\n",
    "        outputs = model(tokens, prefix, mask)\n",
    "        logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens2.flatten(), ignore_index=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "        progress.update()\n",
    "    progress.close()\n",
    "    if epoch % args.save_every == 0 or epoch == epochs - 1 + start_epoch:\n",
    "        model_path = os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\")\n",
    "        args_path = model_path.replace('.pt', '_args.pkl')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        with open(args_path, 'wb') as f:\n",
    "            pickle.dump(args, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "from typing import  Union\n",
    "from dataset import ClipGPTFlickr8kDataset\n",
    "from models import ClipCaptionModel, ClipCaptionPrefix, MappingType\n",
    "from args import DemoArgs\n",
    "from bleu import belu_score\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def load_model(model_path):\n",
    "    '''load model from path'''\n",
    "    epoch_number = int(model_path.split('-')[-1].split('.')[0]) + 1\n",
    "    args_path = model_path.replace('.pt', '_args.pkl')\n",
    "    with open(args_path, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    model = ClipCaptionPrefix(args.prefix_length, args.lang , clip_length=args.prefix_length_clip, prefix_size=512, num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    return model , args, epoch_number    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoints/english_exp_2-000.pt'\n",
    "model, args, start_epoch = load_model(model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClipGPTFlickr8kDataset(args.data, args.prefix_length,lang= args.lang, normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=False, drop_last=True)\n",
    "\n",
    "dataset2 = ClipGPTFlickr8kDataset('./data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', args.prefix_length,lang= 'arabic', normalize_prefix=args.normalize_prefix)\n",
    "tokenizer = dataset2.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ietr_obj = iter(train_dataloader)\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        tokens, mask, prefix = next(ietr_obj)\n",
    "        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        outputs = model(tokens, prefix, mask)\n",
    "        logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "        preds = torch.argmax(logits, dim=-1).tolist()\n",
    "\n",
    "        for pred in preds:\n",
    "            print(tokenizer.decode(pred))\n",
    "            print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pickle \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from bleu import generate_caption\n",
    "from inference_gpt import load_model\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "# select random 10 images\n",
    "random.seed(42)\n",
    "\n",
    "# model_path = './checkpoints/english_exp_1-029.pt'\n",
    "model_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "\n",
    "k = 10\n",
    "\n",
    "if 'english' in model_path:\n",
    "    lang = 'english'\n",
    "if 'arabic' in model_path:\n",
    "    lang = 'arabic'\n",
    "\n",
    "data_path = f'./data/embeddings/{lang}_CLIP-ViT-B-32_embeddings.pkl'\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "image_ids = [data['captions'][i]['image_id'] for i in range(len(data['captions']))]\n",
    "unique_image_ids = np.unique(image_ids, return_index=True)[1]\n",
    "n = random.sample(list(unique_image_ids), k)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "logit_scale = clip_model.logit_scale.exp().float().to('cpu')\n",
    "\n",
    "multilingual_clip_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "multilingual_tokenizer = transformers.AutoTokenizer.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "multilingual_tokenizer.pad_token = multilingual_tokenizer.eos_token\n",
    "\n",
    "\n",
    "pretrained_model, prefix_length = load_model(model_path)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "if lang == 'arabic':\n",
    "    pretrained_tokenizer = AutoTokenizer.from_pretrained(\"akhooli/gpt2-small-arabic\")\n",
    "if lang == 'english':\n",
    "    pretrained_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
    "\n",
    "\n",
    "image_embeddings = data['clip_embedding'][n].float()\n",
    "sample_captions = [data['captions'][i]['caption'] for i in n]\n",
    "image_ids = [data['captions'][i]['image_id'] for i in n]\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(image_ids))):\n",
    "    image_path = f'./data/images/{image_ids[i]}'\n",
    "    prediction = generate_caption(image_path, pretrained_model ,preprocess, clip_model, pretrained_tokenizer, prefix_length, device)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = multilingual_clip_model.forward(sample_captions, multilingual_tokenizer).float()\n",
    "    predicted_embeddings = multilingual_clip_model.forward(predictions, pretrained_tokenizer).float()\n",
    "\n",
    "    image_features = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_features  = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    predicted_text_features  = predicted_embeddings / predicted_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    true_similarities = []\n",
    "    pred_similarities = []\n",
    "    true_captions = []\n",
    "    pred_captions = []\n",
    "    for i in range(len(image_ids)):\n",
    "        true_similarity = logit_scale * (image_features[i]* text_features[i]).sum()\n",
    "        pred_similarity = logit_scale * (image_features[i]* predicted_text_features[i]).sum()\n",
    "        true_similarities.append(true_similarity.item())\n",
    "        pred_similarities.append(pred_similarity.item())\n",
    "        true_captions.append(sample_captions[i])\n",
    "        pred_captions.append(predictions[i])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'image_id': image_ids,\n",
    "        'true_similarity': true_similarities,\n",
    "        'predicted_similarity': pred_similarities,\n",
    "        'true_caption': true_captions,\n",
    "        'predicted_caption': pred_captions\n",
    "        })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Run Inference On Sample Images ################\n",
    "import inference_gpt\n",
    "ckpt_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "inference_gpt.main(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import belu_score\n",
    "\n",
    "model_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "belu_score(model_path)\n",
    "model_path = './checkpoints/english_exp_1-029.pt'\n",
    "belu_score(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a1108c086910f85ca4baff738b45dae52df791633b9cd1b62a05e2975e2a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
