{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "tracker_path = './checkpoints/english_train_dev_tracker.pkl'\n",
    "\n",
    "with open(tracker_path, 'rb') as f:\n",
    "    tracker = pickle.load(f)\n",
    "\n",
    "tracker_keys = ['loss_per_batch', 'loss_per_epoch', 'time_per_epoch', 'time_per_batch', 'bleu_per_epoch', 'loss_per_batch_dev', 'loss_per_epoch_dev', 'time_per_epoch_dev', 'time_per_batch_dev', 'bleu_per_epoch_dev']\n",
    "\n",
    "#sample loss per batch for training and dev set every x batches\n",
    "x = 100\n",
    "sampled_loss_per_batch = tracker['loss_per_batch'][::x]\n",
    "x = 100\n",
    "sampled_loss_per_batch_dev = tracker['loss_per_batch_dev'][::x]\n",
    "\n",
    "#plot loss per batch for training and dev set side by side \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.plot(sampled_loss_per_batch)\n",
    "ax1.set_title('Loss per batch for training set')\n",
    "ax1.set_xlabel('Batch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2.plot(sampled_loss_per_batch_dev)\n",
    "ax2.set_title('Loss per batch for dev set')\n",
    "ax2.set_xlabel('Batch')\n",
    "ax2.set_ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import BLEU\n",
    "\n",
    "args_path = './checkpoints/english_train_dev_args.json'\n",
    "model_path = './checkpoints/english_train_dev-008.pt'\n",
    "\n",
    "bleu_obj = BLEU(args_path, model_path, use_eval_data=True)\n",
    "\n",
    "n = 600\n",
    "score_on_eval = bleu_obj.calculate_bleu(n=n)\n",
    "\n",
    "print(f'socre on {n} sample from Google dev set: {score_on_eval:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import BLEU\n",
    "\n",
    "args_path = './checkpoints/english_train_dev_args.json'\n",
    "tracker_path = './checkpoints/english_train_dev_tracker.json'\n",
    "model_path = './checkpoints/english_train_dev-008.pt'\n",
    "bleu_obj = BLEU(args_path, model_path, use_eval_data=False)\n",
    "args = bleu_obj.args\n",
    "all_img_paths = bleu_obj.sample_images_paths\n",
    "num_samples = len(all_img_paths)\n",
    "dev_ratio = args.dev_ratio\n",
    "train_size = int((1 - dev_ratio) * num_samples)\n",
    "dev_size = num_samples - train_size\n",
    "train_indices = list(range(train_size))\n",
    "dev_indices = list(range(train_size, num_samples))\n",
    "print(f'number of dev samples: {len(dev_indices)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1_gram_weight = 0.0\n",
    "n_2_gram_weight = 1.0\n",
    "n_3_gram_weight = 0.0\n",
    "n_4_gram_weight = 0.0\n",
    "\n",
    "weights = [n_1_gram_weight, n_2_gram_weight, n_3_gram_weight, n_4_gram_weight]\n",
    "num_samples_to_eval = 50\n",
    "n = dev_indices[:num_samples_to_eval]\n",
    "\n",
    "belu_scores = []\n",
    "for i in range(num_samples_to_eval):\n",
    "    k = [n[i]]\n",
    "    score_on_eval = bleu_obj.calculate_bleu(n=k, ngram_weights=weights)\n",
    "    belu_scores.append(score_on_eval)\n",
    "\n",
    "belu_scores = sorted(belu_scores, reverse=True)\n",
    "for i in range(len(belu_scores)):\n",
    "    print(f'{i+1}th best score: {belu_scores[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import BLEU\n",
    "\n",
    "args_path = './checkpoints/arabic_train_dev_args.json'\n",
    "model_path = './checkpoints/arabic_train_dev-005.pt'\n",
    "bleu_obj = BLEU(args_path, model_path, use_eval_data=False)\n",
    "args = bleu_obj.args\n",
    "all_img_paths = bleu_obj.sample_images_paths\n",
    "num_samples = len(all_img_paths)\n",
    "dev_ratio = args.dev_ratio\n",
    "train_size = int((1 - dev_ratio) * num_samples)\n",
    "dev_size = num_samples - train_size\n",
    "train_indices = list(range(train_size))\n",
    "dev_indices = list(range(train_size, num_samples))\n",
    "print(f'number of dev samples: {len(dev_indices)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1_gram_weight = 0.0\n",
    "n_2_gram_weight = 1.0\n",
    "n_3_gram_weight = 0.0\n",
    "n_4_gram_weight = 0.0\n",
    "\n",
    "weights = [n_1_gram_weight, n_2_gram_weight, n_3_gram_weight, n_4_gram_weight]\n",
    "num_samples_to_eval = 50\n",
    "n = dev_indices[:num_samples_to_eval]\n",
    "\n",
    "belu_scores = []\n",
    "for i in range(num_samples_to_eval):\n",
    "    k = [n[i]]\n",
    "    score_on_eval = bleu_obj.calculate_bleu(n=k, ngram_weights=weights)\n",
    "    belu_scores.append(score_on_eval)\n",
    "\n",
    "belu_scores = sorted(belu_scores, reverse=True)\n",
    "for i in range(len(belu_scores)):\n",
    "    print(f'{i+1}th best score: {belu_scores[i]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Flickr8kDaClipGPTFlickr8kDatasettaset\n",
    "from torch.utils.data import DataLoader\n",
    "from cnn_rnn import DecoderRNN\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "data_path = 'data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = ClipGPTFlickr8kDataset( data_path, 10, lang='arabic')\n",
    "data_loader_train = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "tokenizer = dataset.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 100\n",
    "num_layers =1 \n",
    "num_epochs = 20\n",
    "print_every = 50\n",
    "save_every = 1 \n",
    "vocab_size = tokenizer.vocab_size\n",
    "total_step = math.ceil( len(data_loader_train) / data_loader_train.batch_sampler.batch_size   )\n",
    "\n",
    "decoder = DecoderRNN(  embed_size , hidden_size, vocab_size ,num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "all_params = list(decoder.parameters()) \n",
    "optimizer = torch.optim.Adam( params  = all_params , lr = lr  )\n",
    "\n",
    "\n",
    "device =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_save_path = './saved_models/CLIP_RNN_AR/checkpoint'\n",
    "os.makedirs( model_save_path , exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Save the params needed to created the model :\n",
    "decoder_input_params = {'embed_size' : embed_size , \n",
    "                'hidden_size' : hidden_size , \n",
    "                'num_layers' : num_layers,\n",
    "                'lr' : lr ,\n",
    "                'vocab_size' : vocab_size\n",
    "                }\n",
    "\n",
    "with open(  os.path.join(model_save_path , 'decoder_input_params_12_20_2019.pickle'), 'wb') as handle:\n",
    "    pickle.dump(decoder_input_params, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import torch.utils.data as data\n",
    "for e in range(num_epochs):\n",
    "  for step in range(total_step):\n",
    "    indices = data_loader_train.dataset.get_train_indices()\n",
    "    new_sampler = data.sampler.SubsetRandomSampler( indices )\n",
    "    data_loader_train.batch_sampler.sampler = new_sampler    \n",
    "    images,captions = next(iter(data_loader_train))    \n",
    "    images , captions = images.to(device) , captions.to(device)\n",
    "    encoder , decoder = encoder.to(device) , decoder.to(device)\n",
    "    encoder.eval()\n",
    "    decoder.zero_grad()\n",
    "    features = encoder(images)\n",
    "    output = decoder( features , captions )    \n",
    "    loss = criterion( output.view(-1, vocab_size) , captions.view(-1) )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    stat_vals = 'Epochs [%d/%d] Step [%d/%d] Loss [%.4f] ' %( e+1,num_epochs,step,total_step,loss.item() )\n",
    "    if step % print_every == 0 :\n",
    "      print(stat_vals)\n",
    "      sys.stdout.flush()\n",
    "    if e % save_every == 0:\n",
    "      torch.save( encoder.state_dict() ,  os.path.join( model_save_path , 'encoderdata_{}.pkl'.format(e+1) ) )\n",
    "      torch.save( decoder.state_dict() ,  os.path.join( model_save_path , 'decoderdata_{}.pkl'.format(e+1) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_gpt import Inference\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from bleu import BLEU\n",
    "\n",
    "english_args_path = './checkpoints/english_no_subset_args.json'\n",
    "english_model_path = './checkpoints/english_no_subset-015.pt'\n",
    "\n",
    "# arabic_args_path = './checkpoints/arabic_no_subset_args.json'\n",
    "# arabic_model_path = './checkpoints/arabic_no_subset-029.pt'\n",
    "\n",
    "\n",
    "bleu_obj = BLEU(english_args_path, english_model_path, use_eval_data=False)\n",
    "# bleu_obj = BLEU(arabic_args_path, arabic_model_path, use_eval_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_obj.calculate_bleu(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f'Refrence caption: {\" \".join(bleu_obj.references[i][0])}')\n",
    "    print(f'Generated caption: {\" \".join(bleu_obj.candidates[i])}')\n",
    "    print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_gpt import Inference\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from bleu import BLEU\n",
    "\n",
    "# arabic_args_path = './checkpoints/arabic_no_subset_args.json'\n",
    "# arabic_model_path = './checkpoints/arabic_no_subset-029.pt'\n",
    "english_args_path = './checkpoints/english_no_subset_args.json'\n",
    "english_model_path = './checkpoints/english_no_subset-015.pt'\n",
    "\n",
    "# inf_obj_arabic = Inference(arabic_args_path, arabic_model_path)\n",
    "# inf_obj_english = Inference(english_args_path, english_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_caption, sample_image_path = inf_obj_arabic.generate_caption()\n",
    "img = Image.open(sample_image_path)\n",
    "print(sample_caption)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_caption, sample_image_path = inf_obj_english.generate_caption()\n",
    "img = Image.open(sample_image_path)\n",
    "print(sample_caption)\n",
    "plt.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = [6]\n",
    "trackers = []\n",
    "lang = 'english'\n",
    "\n",
    "for epoch in epochs:\n",
    "    if epoch == 0:\n",
    "        tracker_path = f'./checkpoints/{lang}_no_subset_tracker.pkl'\n",
    "    else:\n",
    "        tracker_path = f'./checkpoints/{lang}_no_subset_tracker-{epoch:03d}.pkl'\n",
    "    with open(tracker_path, 'rb') as f:\n",
    "        trackers.append(pickle.load(f))\n",
    "t = trackers[0]\n",
    "# reduce the len of loss_per_batch by taking the mean of every x batches\n",
    "loss_per_batch = t['loss_per_batch']\n",
    "x = 10\n",
    "loss_per_batch = [sum(loss_per_batch[i:i+x])/x for i in range(0, len(loss_per_batch), x)]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the losses\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(loss_per_batch, label='train loss')\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title(f'Loss per batch for {lang} model')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['bleu_per_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "epochs = [15]\n",
    "trackers = []\n",
    "lang = 'arabic'\n",
    "\n",
    "for epoch in epochs:\n",
    "    if epoch == 0:\n",
    "        tracker_path = f'./checkpoints/{lang}_no_subset_tracker.pkl'\n",
    "    else:\n",
    "        tracker_path = f'./checkpoints/{lang}_no_subset_tracker-{epoch:03d}.pkl'\n",
    "    with open(tracker_path, 'rb') as f:\n",
    "        trackers.append(pickle.load(f))\n",
    "t = trackers[0]\n",
    "# reduce the len of loss_per_batch by taking the mean of every x batches\n",
    "loss_per_batch = t['loss_per_batch']\n",
    "x = 10\n",
    "loss_per_batch = [sum(loss_per_batch[i:i+x])/x for i in range(0, len(loss_per_batch), x)]\n",
    "\n",
    "\n",
    "\n",
    "# Plot the losses\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax.plot(loss_per_batch, label='train loss')\n",
    "ax.set_xlabel('Batch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title(f'Loss per batch for {lang} model')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t['bleu_per_epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ClipGPTFlickr8kDatasetBilingual\n",
    "\n",
    "dataset = ClipGPTFlickr8kDatasetBilingual()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_arabic, mask_arabic, tokens_english, mask_english, prefix = dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'tokens_arabic shape: {tokens_arabic.shape}')\n",
    "print(f'mask_arabic shape: {mask_arabic.shape}')\n",
    "print(f'tokens_english shape: {tokens_english.shape}')\n",
    "print(f'mask_english shape: {mask_english.shape}')\n",
    "print(f'prefix shape: {prefix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "arabic_embeddings_path = 'data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl'\n",
    "english_embeddings_path = 'data/embeddings/english_CLIP-ViT-B-32_embeddings.pkl'\n",
    "\n",
    "with open(arabic_embeddings_path, 'rb') as f:\n",
    "    arabic_embeddings = pickle.load(f)\n",
    "\n",
    "with open(english_embeddings_path, 'rb') as f:\n",
    "    english_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_embeddings['clip_embedding'].shape, english_embeddings['clip_embedding'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arabic_embeddings['captions']) , len(english_embeddings['captions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_embeddings['captions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_captions = [i['caption'] for i in english_embeddings['captions']]\n",
    "english_image_ids = [i['image_id'] for i in english_embeddings['captions']]\n",
    "english_clip_embeddings = [i['clip_embedding'] for i in english_embeddings['captions']]\n",
    "\n",
    "arabic_captions = [i['caption'] for i in arabic_embeddings['captions']]\n",
    "arabic_image_ids = [i['image_id'] for i in arabic_embeddings['captions']]\n",
    "arabic_clip_embeddings = [i['clip_embedding'] for i in arabic_embeddings['captions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "multilingual_captions = []\n",
    "len_multilingual_data = len(english_captions)\n",
    "\n",
    "\n",
    "for english_index in range(len_multilingual_data):\n",
    "    img_id = english_image_ids[english_index] \n",
    "    clip_embdding = english_clip_embeddings[english_index]\n",
    "\n",
    "    # get all of the arabic captions for the same image\n",
    "    arabic_indices = [i for i, x in enumerate(arabic_image_ids) if x == img_id]\n",
    "    #select one of them randomly\n",
    "    arabic_index = np.random.choice(arabic_indices)\n",
    "\n",
    "    arabic_caption = arabic_captions[arabic_index]\n",
    "    english_caption = english_captions[english_index]\n",
    "\n",
    "    multilingual_item = {\n",
    "        'image_id': img_id,\n",
    "        'clip_embedding': clip_embdding,\n",
    "        'arabic_caption': arabic_caption,\n",
    "        'english_caption': english_caption\n",
    "    }\n",
    "\n",
    "    multilingual_captions.append(multilingual_item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_data = {\n",
    "    'captions': multilingual_captions,\n",
    "    'clip_embedding': english_embeddings['clip_embedding']\n",
    "}\n",
    "\n",
    "# save the data\n",
    "with open('data/embeddings/multilingual_CLIP-ViT-B-32_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(multilingual_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import BLEU\n",
    "\n",
    "args_path = 'checkpoints/arabic_no_subset_args.json'\n",
    "model_path = 'checkpoints/arabic_no_subset-009.pt'\n",
    "\n",
    "bleu_obj = BLEU(args_path, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = bleu_obj.sample_images_paths\n",
    "captions = bleu_obj.captions_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "image_path = paths[i]\n",
    "caption = captions[i]\n",
    "generated_caption = bleu_obj.generate_caption(image_path)\n",
    "\n",
    "print(f'Generted caption: {generated_caption}')\n",
    "print(f'Reference caption: {caption}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_obj.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "tarcker_path = 'checkpoints/arabic_subset_0.5_tracker.pkl'\n",
    "with open(tarcker_path, 'rb') as f:\n",
    "    tracker = pickle.load(f)\n",
    "\n",
    "tracker.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in tracker.items():\n",
    "    print(f'{key}:\\t{len(val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = AutoModelForCausalLM.from_pretrained(\"elgeish/gpt2-medium-arabic-poetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = mm(inputs_embeds=torch.ones(1, 1, dtype=torch.int64), labels=torch.ones((1,1)), attention_mask=torch.ones(1, 1, dtype=torch.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x =model(inputs_embeds=torch.ones(1, 1, dtype=torch.int64), attention_mask=torch.ones(1, 1, dtype=torch.int64), labels=torch.ones((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding size of the GPT2 model\n",
    "gpt_embedding_size = model.transformer.wte.weight.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import BLEU\n",
    "\n",
    "model_path = './checkpoints/arabic_exp_3-030.pt'\n",
    "\n",
    "\n",
    "bleu_obj = BLEU()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "tracker_path = './checkpoints/arabic_subset_0.1_tracker.pkl'\n",
    "\n",
    "with open(tracker_path, 'rb') as f:\n",
    "    tracker = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in tracker.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loss Per Epoch: {tracker[\"loss_per_epoch\"]}')\n",
    "print(f'Time Per Epoch: {tracker[\"time_per_epoch\"]}')\n",
    "print(f'BLEU Per Epoch: {tracker[\"bleu_per_epoch\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "plt.plot(tracker['loss_per_batch'], label='train_loss')\n",
    "plt.plot(tracker['time_per_batch'], label='time_per_batch')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
