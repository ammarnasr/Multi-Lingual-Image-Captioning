{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "from typing import  Union\n",
    "from dataset import ClipGPTFlickr8kDataset\n",
    "from models import ClipCaptionModel, ClipCaptionPrefix, MappingType\n",
    "from args import DemoArgs\n",
    "from bleu import belu_score\n",
    "\n",
    "\n",
    "args = DemoArgs()\n",
    "lr = 2e-5\n",
    "warmup_steps = 5000\n",
    "output_dir= args.out_dir\n",
    "output_prefix = args.output_prefix\n",
    "start_epoch = 0\n",
    "device = torch.device('cuda:0')\n",
    "batch_size = args.bs\n",
    "epochs = args.epochs\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model = ClipCaptionPrefix(args.prefix_length, lang=args.lang , clip_length=args.prefix_length_clip, prefix_size=args.prefix_dim, num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "dataset = ClipGPTFlickr8kDataset(args.data, args.prefix_length,lang= args.lang, normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=False, drop_last=True)\n",
    "dataset2 = ClipGPTFlickr8kDataset('./data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', args.prefix_length,lang= 'arabic', normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader2 = DataLoader(dataset2, batch_size=3, shuffle=False, drop_last=True)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=epochs * len(train_dataloader))\n",
    "for epoch in range(start_epoch, epochs+start_epoch):\n",
    "    ietr_obj = iter(train_dataloader)\n",
    "    ietr_obj2 = iter(train_dataloader2)\n",
    "    print(f\">>> Training epoch {epoch} out of {epochs+start_epoch}\")\n",
    "    sys.stdout.flush()\n",
    "    number_of_batches = len(train_dataloader)\n",
    "    progress = tqdm(total=len(train_dataloader), desc=output_prefix)\n",
    "    for idx in range(number_of_batches):\n",
    "        tokens, mask, prefix = next(ietr_obj)\n",
    "        tokens= tokens[0:3]\n",
    "        mask = mask[0:3]\n",
    "        prefix = prefix[0:3]\n",
    "        tokens2, _, prefix2 = next(ietr_obj2)\n",
    "        model.zero_grad()\n",
    "        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        tokens2 = tokens2.to(device)\n",
    "        outputs = model(tokens, prefix, mask)\n",
    "        logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "        loss = nnf.cross_entropy(logits.reshape(-1, logits.shape[-1]), tokens2.flatten(), ignore_index=0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress.set_postfix({\"loss\": loss.item()})\n",
    "        progress.update()\n",
    "    progress.close()\n",
    "    if epoch % args.save_every == 0 or epoch == epochs - 1 + start_epoch:\n",
    "        model_path = os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}.pt\")\n",
    "        args_path = model_path.replace('.pt', '_args.pkl')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        with open(args_path, 'wb') as f:\n",
    "            pickle.dump(args, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as nnf\n",
    "from torch.utils.data import  DataLoader\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pickle\n",
    "import json\n",
    "from typing import  Union\n",
    "from dataset import ClipGPTFlickr8kDataset\n",
    "from models import ClipCaptionModel, ClipCaptionPrefix, MappingType\n",
    "from args import DemoArgs\n",
    "from bleu import belu_score\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "def load_model(model_path):\n",
    "    '''load model from path'''\n",
    "    epoch_number = int(model_path.split('-')[-1].split('.')[0]) + 1\n",
    "    args_path = model_path.replace('.pt', '_args.pkl')\n",
    "    with open(args_path, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    model = ClipCaptionPrefix(args.prefix_length, args.lang , clip_length=args.prefix_length_clip, prefix_size=512, num_layers=args.num_layers, mapping_type=args.mapping_type)\n",
    "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
    "    return model , args, epoch_number    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './checkpoints/english_exp_2-000.pt'\n",
    "model, args, start_epoch = load_model(model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size is 40455\n",
      "Data size is 24273\n"
     ]
    }
   ],
   "source": [
    "dataset = ClipGPTFlickr8kDataset(args.data, args.prefix_length,lang= args.lang, normalize_prefix=args.normalize_prefix)\n",
    "train_dataloader = DataLoader(dataset, batch_size=5, shuffle=False, drop_last=True)\n",
    "\n",
    "dataset2 = ClipGPTFlickr8kDataset('./data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', args.prefix_length,lang= 'arabic', normalize_prefix=args.normalize_prefix)\n",
    "tokenizer = dataset2.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ietr_obj = iter(train_dataloader)\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        tokens, mask, prefix = next(ietr_obj)\n",
    "        tokens, mask, prefix = tokens.to(device), mask.to(device), prefix.to(device, dtype=torch.float32)\n",
    "        outputs = model(tokens, prefix, mask)\n",
    "        logits = outputs.logits[:, dataset.prefix_length - 1: -1]\n",
    "        preds = torch.argmax(logits, dim=-1).tolist()\n",
    "\n",
    "        for pred in preds:\n",
    "            print(tokenizer.decode(pred))\n",
    "            print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pickle \n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from bleu import generate_caption\n",
    "from inference_gpt import load_model\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "# select random 10 images\n",
    "random.seed(42)\n",
    "\n",
    "# model_path = './checkpoints/english_exp_1-029.pt'\n",
    "model_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "\n",
    "k = 10\n",
    "\n",
    "if 'english' in model_path:\n",
    "    lang = 'english'\n",
    "if 'arabic' in model_path:\n",
    "    lang = 'arabic'\n",
    "\n",
    "data_path = f'./data/embeddings/{lang}_CLIP-ViT-B-32_embeddings.pkl'\n",
    "with open(data_path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "image_ids = [data['captions'][i]['image_id'] for i in range(len(data['captions']))]\n",
    "unique_image_ids = np.unique(image_ids, return_index=True)[1]\n",
    "n = random.sample(list(unique_image_ids), k)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "logit_scale = clip_model.logit_scale.exp().float().to('cpu')\n",
    "\n",
    "multilingual_clip_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "multilingual_tokenizer = transformers.AutoTokenizer.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "multilingual_tokenizer.pad_token = multilingual_tokenizer.eos_token\n",
    "\n",
    "\n",
    "pretrained_model, prefix_length = load_model(model_path)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "if lang == 'arabic':\n",
    "    pretrained_tokenizer = AutoTokenizer.from_pretrained(\"akhooli/gpt2-small-arabic\")\n",
    "if lang == 'english':\n",
    "    pretrained_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "pretrained_tokenizer.pad_token = pretrained_tokenizer.eos_token\n",
    "\n",
    "\n",
    "image_embeddings = data['clip_embedding'][n].float()\n",
    "sample_captions = [data['captions'][i]['caption'] for i in n]\n",
    "image_ids = [data['captions'][i]['image_id'] for i in n]\n",
    "\n",
    "predictions = []\n",
    "for i in tqdm(range(len(image_ids))):\n",
    "    image_path = f'./data/images/{image_ids[i]}'\n",
    "    prediction = generate_caption(image_path, pretrained_model ,preprocess, clip_model, pretrained_tokenizer, prefix_length, device)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = multilingual_clip_model.forward(sample_captions, multilingual_tokenizer).float()\n",
    "    predicted_embeddings = multilingual_clip_model.forward(predictions, pretrained_tokenizer).float()\n",
    "\n",
    "    image_features = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "    text_features  = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "    predicted_text_features  = predicted_embeddings / predicted_embeddings.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    true_similarities = []\n",
    "    pred_similarities = []\n",
    "    true_captions = []\n",
    "    pred_captions = []\n",
    "    for i in range(len(image_ids)):\n",
    "        true_similarity = logit_scale * (image_features[i]* text_features[i]).sum()\n",
    "        pred_similarity = logit_scale * (image_features[i]* predicted_text_features[i]).sum()\n",
    "        true_similarities.append(true_similarity.item())\n",
    "        pred_similarities.append(pred_similarity.item())\n",
    "        true_captions.append(sample_captions[i])\n",
    "        pred_captions.append(predictions[i])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'image_id': image_ids,\n",
    "        'true_similarity': true_similarities,\n",
    "        'predicted_similarity': pred_similarities,\n",
    "        'true_caption': true_captions,\n",
    "        'predicted_caption': pred_captions\n",
    "        })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Run Inference On Sample Images ################\n",
    "import inference_gpt\n",
    "ckpt_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "inference_gpt.main(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleu import belu_score\n",
    "\n",
    "model_path = './checkpoints/arabic_exp_2-045.pt'\n",
    "belu_score(model_path)\n",
    "model_path = './checkpoints/english_exp_1-029.pt'\n",
    "belu_score(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a1108c086910f85ca4baff738b45dae52df791633b9cd1b62a05e2975e2a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
