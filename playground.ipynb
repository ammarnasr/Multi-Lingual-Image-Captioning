{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess\n",
    "from dataset import ClipGPTFlickr8kDataset\n",
    "\n",
    "# Preprocess Data (Data already saved to drive no need to run)\n",
    "# preprocess.create_CLIP_embeddings_for_images(lang='arabic')\n",
    "dataset = ClipGPTFlickr8kDataset('./data/embeddings/arabic_CLIP-ViT-B-32_embeddings.pkl', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edin\\anaconda3\\envs\\mlp\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1248: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:29<00:00,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BLEU for Langauge arabic score is 26.197629218220865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bleu import belu_score\n",
    "\n",
    "\n",
    "model_path = './checkpoints/arabic_exp_1-029.pt'\n",
    "\n",
    "\n",
    "belu_score(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import json\n",
    "import torch\n",
    "import PIL.Image \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import skimage.io as io\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from inference_gpt import load_model, beam_search\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer\n",
    "\n",
    "def prepare_data_for_bleu(file_path, n=200):    \n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sample_image_captions = [item['caption'] for item in data]\n",
    "    sample_image_ids = [item['image_id'] for item in data]\n",
    "    unique_image_ids = list(set(sample_image_ids))\n",
    "    unique_image_ids = unique_image_ids[:n]\n",
    "    \n",
    "    image_ids_occurences = []\n",
    "    for image_id in unique_image_ids:\n",
    "        image_ids_occurences.append([i for i, x in enumerate(sample_image_ids) if x == image_id])\n",
    "    captions_per_image = []\n",
    "    for image_id_occurence in image_ids_occurences:\n",
    "        captions_per_image.append([sample_image_captions[i] for i in image_id_occurence])\n",
    "    sample_images_paths = [os.path.join(sample_images_dir, image_name) for image_name in unique_image_ids]\n",
    "\n",
    "    return captions_per_image, sample_images_paths\n",
    "    \n",
    "    \n",
    "\n",
    "def generate_caption(image_path, model, preprocess, clip_model, tokenizer ,prefix_length,  lang ,device):\n",
    "    image = io.imread(image_path)\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        generated_text_prefix = beam_search(model, tokenizer, embed=prefix_embed, entry_length=10)\n",
    "    return generated_text_prefix\n",
    "\n",
    "\n",
    "file_path = './data/annotations/arabic_captions.json'\n",
    "model_path = './checkpoints/arabic_exp_1-029.pt'\n",
    "sample_images_dir = './data/images/'\n",
    "\n",
    "if 'english' in model_path:\n",
    "    lang = 'english'\n",
    "if 'arabic' in model_path:\n",
    "    lang = 'arabic'\n",
    "\n",
    "sample_image_captions, sample_images_paths = prepare_data_for_bleu(file_path)\n",
    "\n",
    "\n",
    "# Load the CLIP model\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Load the GPT model Tokenizer\n",
    "if lang == 'arabic':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"akhooli/gpt2-small-arabic\")\n",
    "if lang == 'english':\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load the GPT model\n",
    "model, prefix_length = load_model(model_path)\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = []\n",
    "references = []\n",
    "for i in tqdm(range(len(sample_images_paths))):\n",
    "    image_path = sample_images_paths[i]\n",
    "    prediction = generate_caption(image_path, model,preprocess, clip_model, tokenizer, prefix_length, lang, device)\n",
    "    candidates.append(prediction.split(' '))\n",
    "    references.append([r.split(' ') for r in sample_image_captions[i]])\n",
    "\n",
    "score = corpus_bleu(references, candidates) *100\n",
    "print(f'The BLEU score is {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#data\n",
    "file_path = './data/annotations/arabic_captions.json'\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "sample_image_captions = [item['caption'] for item in data]\n",
    "sample_image_ids = [item['image_id'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15a1108c086910f85ca4baff738b45dae52df791633b9cd1b62a05e2975e2a2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
